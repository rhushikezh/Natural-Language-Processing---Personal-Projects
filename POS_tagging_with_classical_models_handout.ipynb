{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "no_k76e0NZHI"
   },
   "source": [
    "# Task: Part of speech tagging\n",
    "\n",
    "In this task we try to recreate a very rudimentary POS tagger \"from scratch\" using SpaCy and CRF models. \n",
    "\n",
    "(We disregard the fact, that SpaCy has a built in POS tagger for the moment for demonstration purposes.)\n",
    "\n",
    "The input is a tokenized English sentence. The task is to label each word with a part of speech (POS) tag. The tag set, which is identical the [Universal Dependencies project's](https://universaldependencies.org/) basic tag set is the following:\n",
    "\n",
    "- NOUN: noun\n",
    "- VERB: verb\n",
    "- DET: determiner\n",
    "- ADJ: adjective\n",
    "- ADP: adposition (e.g., prepositions)\n",
    "- ADV: adverb\n",
    "- CONJ: conjunction\n",
    "- NUM: numeral\n",
    "- PART: particle (function word that cannot be inflected, has no meaning in\n",
    "  itself and doesn't fit elsewhere, e.g., \"to\")\n",
    "- PRON: pronoun\n",
    "- .: punctuation\n",
    "- X: other\n",
    "\n",
    "The code in this task is an adaptation of the NER code in the sklearn-crfsuite documentation.\n",
    "\n",
    "# The data set\n",
    "\n",
    "__Brown__ corpus: \"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961\" (Wikpedia: Brown Corpus)\n",
    "\n",
    "Let's download and inspect the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:20.712982Z",
     "start_time": "2019-11-12T09:32:16.093693Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "KAPwh8mmNZHM",
    "outputId": "09c0b6f3-d1cd-45d4-8d81-e3b3d32cef20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/goldenswan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:21.291370Z",
     "start_time": "2019-11-12T09:32:20.723897Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "3kSgq4e0NZHi",
    "outputId": "bc8efdec-092d-4cde-c35d-cc16de45da7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/goldenswan/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:21.352042Z",
     "start_time": "2019-11-12T09:32:21.297210Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "9tT1DBTtNZHu",
    "outputId": "9c1ed6f2-d1b0-4b54-8497-3864785a115a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:25.131849Z",
     "start_time": "2019-11-12T09:32:21.365202Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UuwBB-XRNZH6",
    "outputId": "10da7445-8b1f-43a1-eed0-2a36ef304951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8VYZmFCsNTa"
   },
   "source": [
    "From the brown the object provided by NLTK we will work with the tagged sentence list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:25.163453Z",
     "start_time": "2019-11-12T09:32:25.136038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bh0wAJkWNnlV",
    "outputId": "d314dffa-af1b-4a35-9859-2e51f48245ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'),\n",
       "  ('Fulton', 'NOUN'),\n",
       "  ('County', 'NOUN'),\n",
       "  ('Grand', 'ADJ'),\n",
       "  ('Jury', 'NOUN'),\n",
       "  ('said', 'VERB'),\n",
       "  ('Friday', 'NOUN'),\n",
       "  ('an', 'DET'),\n",
       "  ('investigation', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  (\"Atlanta's\", 'NOUN'),\n",
       "  ('recent', 'ADJ'),\n",
       "  ('primary', 'NOUN'),\n",
       "  ('election', 'NOUN'),\n",
       "  ('produced', 'VERB'),\n",
       "  ('``', '.'),\n",
       "  ('no', 'DET'),\n",
       "  ('evidence', 'NOUN'),\n",
       "  (\"''\", '.'),\n",
       "  ('that', 'ADP'),\n",
       "  ('any', 'DET'),\n",
       "  ('irregularities', 'NOUN'),\n",
       "  ('took', 'VERB'),\n",
       "  ('place', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('The', 'DET'),\n",
       "  ('jury', 'NOUN'),\n",
       "  ('further', 'ADV'),\n",
       "  ('said', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('term-end', 'NOUN'),\n",
       "  ('presentments', 'NOUN'),\n",
       "  ('that', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('City', 'NOUN'),\n",
       "  ('Executive', 'ADJ'),\n",
       "  ('Committee', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('which', 'DET'),\n",
       "  ('had', 'VERB'),\n",
       "  ('over-all', 'ADJ'),\n",
       "  ('charge', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('election', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('``', '.'),\n",
       "  ('deserves', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('praise', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('thanks', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('City', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Atlanta', 'NOUN'),\n",
       "  (\"''\", '.'),\n",
       "  ('for', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('manner', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('which', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('election', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('conducted', 'VERB'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = brown.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:28.899336Z",
     "start_time": "2019-11-12T09:32:25.166107Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "L2oUMrTpasZY",
    "outputId": "d644b14c-77a1-4044-deb1-02a127bd31ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgvacV45sNTz"
   },
   "source": [
    "We divide our data set into a train and a valid part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:28.905030Z",
     "start_time": "2019-11-12T09:32:28.901963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jPvFic-atE6S"
   },
   "outputs": [],
   "source": [
    "valid_sents = sents[:5734]\n",
    "train_sents = sents[5734:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zO8hXBHHPR4f"
   },
   "source": [
    "# Feature template\n",
    "\n",
    "Since the plan is to build a CRF model, we need a __feature template__, which generates features for a word in a sentence (our sequence in the sequence tagging task). We use spaCy for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:36.258620Z",
     "start_time": "2019-11-12T09:32:28.906793Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Spacy install, load and such stuff\n",
    "\n",
    "#Import\n",
    "import spacy\n",
    "#By model load, please deactivate unnecessary pipeline elements!\n",
    "\n",
    "en = spacy.load(\"en_core_web_sm\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TC4f13w9sNUJ"
   },
   "source": [
    "We write a function which generates features for a token in a sentence, which is already a spaCy document. The feature vector is represented as a `dict` mapping feature names to their values.\n",
    "\n",
    "The desired **feature set for a token is**:\n",
    "\n",
    "- `bias`: A constant value of 1 as an input\n",
    "- `token.lower`: the lowercased textual form of the token\n",
    "- `token.suffix`: the textual form of the token's suffix as defined by SpaCy,\n",
    "- `token.prefix`: the textual form of the token's prefix as defined by SpaCy,\n",
    "- `token.is_upper`: boolean value indicating if the token is uppercase,\n",
    "- `token.is_title`: boolean value indicating if the token is a title,\n",
    "- `token.is_digit`: boolean value indicating if the token consists of numbers.\n",
    "\n",
    "These are only the `Token`'s own properties, but they represent no context.\n",
    "\n",
    "We would like to include information about  the previous and next words, as well as indicating if the `Token` is the beginning or the end of sentence.\n",
    "\n",
    "The **contextual features** should be:\n",
    " \n",
    "- `-1:token.lower`: What is the lowercase textual form of the previous token?,\n",
    "- `-1:token.is_title`: Is the previous token a title?,\n",
    "- `-1:token.is_upper`: Is the previous token uppercase?,\n",
    "- `+1:token.lower`: What is the lowercase textual form of the next token?,\n",
    "- `+1:token.is_title`: Is the next token a title?,\n",
    "- `+1:token.is_upper`: Is the next token uppercase?,\n",
    "- `BOS`: Boolean value indicating if the token is the beginning of a sentence,\n",
    "- `EOS`: Boolean value indicating if the token is the end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:37.930451Z",
     "start_time": "2019-11-12T09:32:37.909065Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SKz9zT8bsNUL"
   },
   "outputs": [],
   "source": [
    "def token2features(sent, i):\n",
    "    \"\"\"Return a feature dict for a token. \n",
    "    sent is a spaCy Doc containing a sentence, i is the token's index in it.\n",
    "    \"\"\"\n",
    "\n",
    "    word = sent[i][0] \n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvwL0hF3sNUS"
   },
   "source": [
    "For training, we will also need functions to generate feature dict and label lists for sentences in our training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:37.958184Z",
     "start_time": "2019-11-12T09:32:37.936592Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZLW80wtksNUU"
   },
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    \"Return a list of feature dicts for a sentence in the data set.\"\n",
    "    # Create a doc by instantiating a Doc class and iterating through the sentence token by token.\n",
    "    # Please bear in mind, that Brown has token-POS pairs, latter one we don't need here...\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plese use the above defined token2features function on each token to generate the features\n",
    "    # For the whole sentence!\n",
    "    sent_features= [token2features(sent, i) for i in range(len(sent))]\n",
    "    \n",
    "    return sent_features\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \n",
    "    #Please create / filter only the labels for given sentence!\n",
    "    labels= [label for word, label in sent]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBoPuzeMsNUa"
   },
   "source": [
    "Sanity check: let's see the values for the first 2 tokens in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:37.997140Z",
     "start_time": "2019-11-12T09:32:37.966347Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "UcqvDIJofccv",
    "outputId": "7e47bfb1-346c-4a04-82d7-ead92f35050d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'The', 'word[-2:]': 'he', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': 'fulton', '+1:word.istitle()': True, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'fulton', 'word[-3:]': 'ton', 'word[-2:]': 'on', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, '-1:word.lower()': 'the', '-1:word.istitle()': True, '-1:word.isupper()': False, '+1:word.lower()': 'county', '+1:word.istitle()': True, '+1:word.isupper()': False}]\n",
      "['DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(sent2features(sents[0])[:2])\n",
    "print(sent2labels(sents[0])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsoK0-GNfzt5"
   },
   "source": [
    "# Putting the data into final form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylfst7VGsNUl"
   },
   "source": [
    "Everything is ready to generate the training data in the form which is usable for the CRFsuite. Note that our inputs and labels will be  2-level representations, lists of lists, because we deal with token sequences (sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:33:02.066506Z",
     "start_time": "2019-11-12T09:32:38.005545Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Wfqa0feYgspT",
    "outputId": "7503580f-e5a4-4c56-80b2-9c70bbd1cce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.79 s, sys: 805 ms, total: 6.59 s\n",
      "Wall time: 6.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_valid = [sent2features(s) for s in valid_sents]\n",
    "y_valid = [sent2labels(s) for s in valid_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:33:02.072026Z",
     "start_time": "2019-11-12T09:33:02.068258Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "XNFvu0UosNUt",
    "outputId": "9195a73b-b3bb-46af-cc61-1e7e23e2a0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dict for the first token in the first validation sentence:\n",
      "{'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'The', 'word[-2:]': 'he', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': 'fulton', '+1:word.istitle()': True, '+1:word.isupper()': False}\n",
      "Its label:\n",
      "DET\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature dict for the first token in the first validation sentence:\")\n",
    "print(X_valid[0][0])\n",
    "print(\"Its label:\")\n",
    "print(y_valid[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2siQxe9k4ql"
   },
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwyn356ysNU2"
   },
   "source": [
    "We use the super-optimized [CRFsuite](http://www.chokkan.org/software/crfsuite/) via the scikit-learn compatible [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) wrapper to train a CRF model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:33:04.830327Z",
     "start_time": "2019-11-12T09:33:02.073675Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "15POzt86sNSe",
    "outputId": "c1ba53b0-e73a-46c9-b09e-53e5f848ed48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tabulate in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from sklearn_crfsuite) (0.8.9)\n",
      "Requirement already satisfied: six in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from sklearn_crfsuite) (4.64.0)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.9-cp39-cp39-macosx_10_9_x86_64.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.9 sklearn-crfsuite-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:33:04.883741Z",
     "start_time": "2019-11-12T09:33:04.836395Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WkX57BFDklEL"
   },
   "outputs": [],
   "source": [
    "# Please import and train an averaged perceptron model from CRFsuite and use it's custom metrics, \n",
    "# especially the multiple forms of accuracy score to evaluate the model!\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "# especially the multiple forms of accuracy score to evaluate the model!\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='ap',\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=False,\n",
    "    epsilon=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722167471987743"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please draw some conclusion if this model is \"good enough\" \n",
    "# in your view if you take token level and sentence level metrics into account!\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "\n",
    "y_pred = crf.predict(X_valid)\n",
    "metrics.flat_f1_score(y_valid, y_pred,average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63p9RtDhsNU_"
   },
   "source": [
    "Let's instantiate and fit our model. CRFsuite implements several learning methods, here we use \"ap\", i.e., averaged perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "li8CXg67sNVc"
   },
   "source": [
    "# Demonstration\n",
    "\n",
    "Just for the fun, we can try out the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:35:17.983727Z",
     "start_time": "2019-11-12T09:35:17.965723Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JHoYAGHFsNVe"
   },
   "outputs": [],
   "source": [
    "def predict_tags(sent):\n",
    "    \"\"\"Predict tags for a sentence.\n",
    "    sent is a string.\n",
    "    \"\"\"\n",
    "    doc = en(sent)\n",
    "    return crf.predict([[token2features(doc, i) for i in range(len(doc))]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:35:37.676093Z",
     "start_time": "2019-11-12T09:35:17.986500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "Ya59xso_z7uj",
    "outputId": "401e3223-c177-420b-8d2e-0120707dac1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence to tag or press return to quit:\n",
      "\n",
      "\n",
      "Empty input received -- bye!\n"
     ]
    }
   ],
   "source": [
    " while True:\n",
    "        sent = input(\"\\nEnter a sentence to tag or press return to quit:\\n\")\n",
    "        if sent:\n",
    "            print(predict_tags(sent))\n",
    "        else:\n",
    "            print(\"\\nEmpty input received -- bye!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "POS_tagging_with_classical_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
