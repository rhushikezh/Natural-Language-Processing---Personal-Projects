{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQGvpTjNl7q-"
   },
   "source": [
    "# Using the SpaCy pipeline\n",
    "\n",
    "This task is aiming to demonstrate the tokenization capabilites of [SpaCy](https://spacy.io/), as well as to serve as an introduction to the pipeline's capabilities combined with [rule based matching](https://spacy.io/usage/rule-based-matching).\n",
    "\n",
    "Our goal will be to process the demonstration text, as well as to correct for some peculiarities, like special pronunciation marks, wide-spread abbreviations and foreign language insertions into our text.\n",
    "\n",
    "It is mandatory, to stick to SpaCy based pipeline operations so as to make our analysis reproducible by running the pipeline on other texts presumably coming from the same corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hghIi-BYl7rH"
   },
   "source": [
    "## Our demonstration text\n",
    "\n",
    "Original from [Deutsche Sprache](https://de.wikipedia.org/wiki/Deutsche_Sprache) Wikipedia entry - with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:03.801971Z",
     "start_time": "2019-11-08T11:44:03.798661Z"
    },
    "id": "dIFE2QzAl7rI"
   },
   "outputs": [],
   "source": [
    "text= '''Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; abgekürzt dt. oder dtsch.) ist eine westgermanische Sprache.\n",
    "\n",
    "And this is an English sentence inbetween.\n",
    "\n",
    "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8iXMRefl7rL"
   },
   "source": [
    "## Basic usage\n",
    "\n",
    "After installing SpaCy, let us demonstrate it's basic usage by analysing our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:04.978496Z",
     "start_time": "2019-11-08T11:44:03.810849Z"
    },
    "id": "7uI7qyJHl7rM"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tabulate >> /dev/null\n",
    "!pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J_g1Rg4ol7rN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.21.0)\n",
      "Requirement already satisfied: setuptools in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (61.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (21.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/goldenswan/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# Ok, we installed SpaCy, but do we have a model for German?\n",
    "# Something has to be done here to get it!\n",
    "\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sB5vxwBol7rN"
   },
   "outputs": [],
   "source": [
    "# If after the action above, the German model does not load, you may have to restart the runtime.\n",
    "# (in Colab it can be so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.365430Z",
     "start_time": "2019-11-08T11:44:04.981233Z"
    },
    "id": "2_YmX5avl7rO"
   },
   "outputs": [],
   "source": [
    "# Please do the appropriate imports for SpaCy and it's rule based Matcher class and Language!\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.symbols import ORTH, POS, NOUN\n",
    "from spacy.language import Language\n",
    "\n",
    "# Please don't forget to instantiate the language model that we will use later on for analysis\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.385990Z",
     "start_time": "2019-11-08T11:44:06.366784Z"
    },
    "id": "4Pw1FygQl7rP"
   },
   "outputs": [],
   "source": [
    "# And please use the model to analyse the text from above!\n",
    "\n",
    "doc= nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LPjwwHnl7rQ"
   },
   "source": [
    "### Helper functions for nice printout\n",
    "\n",
    "We just define some helper functions for nice printout. Nothing to do here, except to observe the ways one can iterate over a corpus or sentence, as well as the nice output of [Tabulate](https://bitbucket.org/astanin/python-tabulate/src/master/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.395057Z",
     "start_time": "2019-11-08T11:44:06.387362Z"
    },
    "id": "L-936Bpol7rR"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_sentences(doc):\n",
    "    for sentence in doc.sents:\n",
    "        print(sentence,\"\\n\")\n",
    "\n",
    "def print_tokens_for_sentence(doc,sentence_num, stopwords=False):\n",
    "    attribs=[]\n",
    "    for token in list(doc.sents)[sentence_num]:\n",
    "        if token.has_extension(\"is_lemma_stop\"):\n",
    "            if stopwords and token._.is_lemma_stop:\n",
    "                pass\n",
    "            else:\n",
    "                attribs.append([token.text, token.lemma_, token.pos_])\n",
    "        else:\n",
    "            attribs.append([token.text, token.lemma_, token.pos_])\n",
    "    print(tabulate(attribs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.401099Z",
     "start_time": "2019-11-08T11:44:06.397107Z"
    },
    "id": "0mUQ_Wnrl7rS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; \n",
      "\n",
      "abgekürzt dt. \n",
      "\n",
      "oder dtsch.) ist eine westgermanische Sprache.\n",
      "\n",
      " \n",
      "\n",
      "And this is an English sentence inbetween.\n",
      "\n",
      " \n",
      "\n",
      "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. \n",
      "\n",
      "Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_sentences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.410817Z",
     "start_time": "2019-11-08T11:44:06.404499Z"
    },
    "id": "PJ40jUMhl7rU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  -------------------  -----\n",
      "Außerdem             außerdem             ADV\n",
      "ist                  sein                 AUX\n",
      "sie                  sie                  PRON\n",
      "eine                 ein                  DET\n",
      "Minderheitensprache  Minderheitensprache  NOUN\n",
      "in                   in                   ADP\n",
      "einigen              einiger              DET\n",
      "europäischen         europäisch           ADJ\n",
      "und                  und                  CCONJ\n",
      "außereuropäischen    außereuropäisch      ADJ\n",
      "Ländern              Land                 NOUN\n",
      ",                    --                   PUNCT\n",
      "z.                   z.                   ADP\n",
      "B.                   B.                   NOUN\n",
      "in                   in                   ADP\n",
      "Rumänien             Rumänien             PROPN\n",
      "und                  und                  CCONJ\n",
      "Südafrika            Südafrika            PROPN\n",
      ",                    --                   PUNCT\n",
      "sowie                sowie                CCONJ\n",
      "Nationalsprache      Nationalsprache      NOUN\n",
      "im                   in                   ADP\n",
      "afrikanischen        afrikanisch          ADJ\n",
      "Namibia              Namibia              PROPN\n",
      ".                    --                   PUNCT\n",
      "-------------------  -------------------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KKtxIvyl7rV"
   },
   "source": [
    "## Matching \"zum Beispiel\"\n",
    "\n",
    "We are a bit frustrated, that the standard analysis pipeline does not know, that in German, \"z. B.\" is the abbreviation of \"zum Beispiel\" (like eg. is for \"for example\"), thus we would like to correct this.\n",
    "\n",
    "Our approach is to extend the pipeline and do a matching, whereby we replace the `lemma` form of \"z. B.\" to the appropriate long form.\n",
    "\n",
    "**IMPORTANT** design principle by SpaCy is, that one **always keeps the possibility to restore the original text**, so we are **NOT to modify `token.text`**. In the analysed form, we can do whatever we want.\n",
    "\n",
    "It is typical to add layers to the pipeline which modify the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNSIc9bMl7rW"
   },
   "source": [
    "For our purposes, we will use rule based matching to achieve our goals.\n",
    "\n",
    "A detailed description on rule based matching in SpaCy can be found [here](https://spacy.io/usage/rule-based-matching), or [here](https://medium.com/@ashiqgiga07/rule-based-matching-with-spacy-295b76ca2b68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldkDPpOkl7rW"
   },
   "source": [
    "### Build the matcher\n",
    "\n",
    "With the help of rule based matching we create a matcher that reacts to the presence of \"z. B.\" exactly, then we use this matcher to define a pipeline step, that after matching, replaces the lemmas of the tokens \"z.\" and \"B.\" to  their full written equivalent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.417129Z",
     "start_time": "2019-11-08T11:44:06.412535Z"
    },
    "id": "uGj6hKLKl7rX"
   },
   "outputs": [],
   "source": [
    "zb_matcher = Matcher(nlp.vocab) # Please instantiate a matcher with the appropriate parameters - think about all the words of the corpus...\n",
    "pattern = [{\"text\":\"z.\"}, {\"text\":\"B.\"}] # Please add an appropriate pattern to the matcher to match \"z. B.\"\n",
    "zb_matcher.add('zum Biespiel', [pattern])\n",
    "\n",
    "\n",
    "@Language.component(\"zb_replacer\")\n",
    "def zb_replacer(doc):\n",
    "    matched_spans = []\n",
    "    # Please use the matcher to get matches!\n",
    "    matches = zb_matcher(doc)\n",
    "    # Plsease iterate over the matches!\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end] # get the span of text based on the matches coordinates!\n",
    "        matched_spans.append(span)\n",
    "        print(\"ZB MATCH!!!\")\n",
    "\n",
    "    # Please iterate over matched spans\n",
    "    for match_id in matched_spans:  \n",
    "        match_id[0].lemma_ = \"zum\"\n",
    "        match_id[1].lemma_ = \"Beispiel\"\n",
    "    return doc\n",
    "        # And replace their lemmas to the appropriate ones!\n",
    "        # Please observe, that you don't have the ID of the desired lemmas, just the their string form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd_i-IKfl7rX"
   },
   "source": [
    "### Register it to the pipeline\n",
    "\n",
    "After creating this processing step, we register it to be part of the pipeline and then run our analysis again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.421259Z",
     "start_time": "2019-11-08T11:44:06.418628Z"
    },
    "id": "7K1oVMKjl7rX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.zb_replacer(doc)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plase register the new zb_replacer to the pipeline!\n",
    "# Think about, where to place it!\n",
    "\n",
    "nlp.add_pipe(\"zb_replacer\", first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmYZiRsel7rY"
   },
   "source": [
    "### Re-do the analysis and observe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.442000Z",
     "start_time": "2019-11-08T11:44:06.422574Z"
    },
    "id": "Hh5oBA0fl7rY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.448772Z",
     "start_time": "2019-11-08T11:44:06.443720Z"
    },
    "id": "Q_lvb2gkl7rZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  -------------------  -----\n",
      "Außerdem             außerdem             ADV\n",
      "ist                  sein                 AUX\n",
      "sie                  sie                  PRON\n",
      "eine                 ein                  DET\n",
      "Minderheitensprache  Minderheitensprache  NOUN\n",
      "in                   in                   ADP\n",
      "einigen              einiger              DET\n",
      "europäischen         europäisch           ADJ\n",
      "und                  und                  CCONJ\n",
      "außereuropäischen    außereuropäisch      ADJ\n",
      "Ländern              Land                 NOUN\n",
      ",                    --                   PUNCT\n",
      "z.                   zum                  ADP\n",
      "B.                   Beispiel             NOUN\n",
      "in                   in                   ADP\n",
      "Rumänien             Rumänien             PROPN\n",
      "und                  und                  CCONJ\n",
      "Südafrika            Südafrika            PROPN\n",
      ",                    --                   PUNCT\n",
      "sowie                sowie                CCONJ\n",
      "Nationalsprache      Nationalsprache      NOUN\n",
      "im                   in                   ADP\n",
      "afrikanischen        afrikanisch          ADJ\n",
      "Namibia              Namibia              PROPN\n",
      ".                    --                   PUNCT\n",
      "-------------------  -------------------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcBUAPC6l7rZ"
   },
   "source": [
    "## What are those ugly pronunciation signs doing there?\n",
    "\n",
    "OK, so far so good. Let's observe, what is the problem with the first sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.473528Z",
     "start_time": "2019-11-08T11:44:06.450123Z"
    },
    "id": "ePiLpmall7ra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.484081Z",
     "start_time": "2019-11-08T11:44:06.476136Z"
    },
    "id": "FO-eMTN8l7ra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  -------  -----\n",
      "Die       der      DET\n",
      "deutsche  deutsch  ADJ\n",
      "Sprache   Sprache  NOUN\n",
      "bzw.      bzw.     CCONJ\n",
      "Deutsch   Deutsch  NOUN\n",
      "(         --       PUNCT\n",
      "[         [        PROPN\n",
      "dɔʏ̯t͡ʃ     dɔʏ̯t͡ʃ    ADV\n",
      "]         ]        PROPN\n",
      ";         --       PUNCT\n",
      "--------  -------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqpwzf-Yl7ra"
   },
   "source": [
    "As we can see, poor pipeline can not really cope with the pronunciation markings of the phonetic alphabet, and thus thinks, that the signs are representing a foreign proper noun. \n",
    "\n",
    "We would like to remedy this, and since we do expect further texts from the corpus to contain these inserted phonetics, we would like to match, merge and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6bnbo-el7ra"
   },
   "source": [
    "## Building up matcher for PRONUNCIATION\n",
    "\n",
    "To be more specific, we again first build up a matcher, that aims at the \"square brackets\" markings around the pronunciation. The task is to match everything between square brackets, or to be more specific: **everything that starts with an opening square bracket, and finishes with \";\"**.\n",
    "\n",
    "This matcher can then be used to:\n",
    "\n",
    "1. Merge the resulting matching `span` into one token\n",
    "2. Replace the token's lemma to \"PRONUNCIATION\"\n",
    "\n",
    "For this to be achievable, we have to first register \"PRONUNCIATION\" as part of the vocabulary, moreover mark it as [\"stopword\"](https://en.wikipedia.org/wiki/Stop_words). (More on SpaCy's stopword handling [here](https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936)) See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.497568Z",
     "start_time": "2019-11-08T11:44:06.486489Z"
    },
    "id": "sUq8ae4Ml7rb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.pronunciation_replacer(doc)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please instantiate and build the matcher as before with the appropriate pattern!\n",
    "# Make it so, that the pattern will match ALL future pronunciations, not just the present one!\n",
    "matcher = Matcher(nlp.vocab) \n",
    "pattern = [{\"TEXT\": \"[\"}, {\"IS_ALPHA\": True}, {\"TEXT\": \";\"}]\n",
    "matcher.add('[;', [pattern])\n",
    "\n",
    "\n",
    "\n",
    "# We set the properties for the new word \"PRONUNCIATION\"\n",
    "lex = nlp.vocab['PRONUNCIATION']\n",
    "\n",
    "lex.is_stop = True\n",
    "\n",
    "@Language.component(\"pronunciation_replacer\")\n",
    "def pronunciation_replacer(doc):\n",
    "    \n",
    "    # Using the template above, please build a pronunciation replacer, that\n",
    "    # 1. Gets the matches\n",
    "    # 2. Iterates through them\n",
    "    # 3. Collects them into a list\n",
    "    # 4. initalizes - WITH context manager - a retokenizer\n",
    "    matches = matcher(doc)\n",
    "    matched_pro = []\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        matched_pro.append(span)\n",
    "        print(\"MATCH!!!\")\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in matched_pro:\n",
    "            retokenizer.merge(span)\n",
    "    \n",
    "    # 7. sets the lemma of the merged span to the PRONUNCIATION lemma we created before\n",
    "    for span in matched_pro:\n",
    "        span[0].lemma_ = \"PRONUNCIATION\"\n",
    "        span[0].pos_ = \"NOUN\"\n",
    "\n",
    "    return doc\n",
    "    # 5. goes through the spans\n",
    "    # 6. merges them\n",
    "    # 7. sets the lemma of the merged span to the PRONUNCIATION lemma we created before\n",
    " \n",
    "\n",
    "\n",
    "nlp.add_pipe(\"pronunciation_replacer\", after=\"zb_replacer\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esKixpjjl7rb"
   },
   "source": [
    "### Observing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.623259Z",
     "start_time": "2019-11-08T11:44:06.500096Z"
    },
    "id": "-qnVljrtl7rb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n",
      "--------  -------  -----\n",
      "Die       der      DET\n",
      "deutsche  deutsch  ADJ\n",
      "Sprache   Sprache  NOUN\n",
      "bzw.      bzw.     CCONJ\n",
      "Deutsch   Deutsch  NOUN\n",
      "(         --       PUNCT\n",
      "[         [        PROPN\n",
      "dɔʏ̯t͡ʃ     dɔʏ̯t͡ʃ    ADV\n",
      "]         ]        PROPN\n",
      ";         --       PUNCT\n",
      "--------  -------  -----\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "print_tokens_for_sentence(doc,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXB46tVdl7rc"
   },
   "source": [
    "In the future, we decide, we would not want to include the pronunciation tokens in our view. So we have to mark them as wtopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmIdVtXMl7rc"
   },
   "source": [
    "### Registering PRONUNCIATION as a stopword\n",
    "\n",
    "Stopwords are typically those words, which do not contribute to the meaning of the sentence, are just there for syntactic reasons. There is a vague running list of these for languages. We will use and extend the German one in SpaCy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.627929Z",
     "start_time": "2019-11-08T11:44:06.625286Z"
    },
    "id": "18FTFl7Xl7rc"
   },
   "outputs": [],
   "source": [
    "# import stop words from GERMAN language data\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# Add PRONUNCIATION to stopwords\n",
    "STOP_WORDS.add(\"PRONUNCIATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0T0Boayl7rc"
   },
   "source": [
    "But since we will only be able to manipulate the lemmas of the pronunciation markings, we would have to let SpaCy know, that - in contrast to the default behavior, where stopwords are filtered on `text` level, we would like to have a new property for words, that is based on `lemma` level stopword filtering.\n",
    "\n",
    "For these we will use extensions!\n",
    "\n",
    "For more info please see [here](https://spacy.io/api/token#set_extension)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.635721Z",
     "start_time": "2019-11-08T11:44:06.629472Z"
    },
    "id": "WNb_wMC4l7rc"
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Please define a function (or lambda expression!) that checks if a Token, or its lower case for, \n",
    "# OR it's lemma string is contained it he stopword list above.\n",
    "stop_words_getter = lambda token: token.is_stop or token.lower_ in STOP_WORDS or token.lemma_ in STOP_WORDS\n",
    "\n",
    "# Set the above defined function as a extension for Token under the name \"is_lemma_stop\" as a getter!\n",
    "Token.set_extension('is_lemma_stop', getter=stop_words_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.672287Z",
     "start_time": "2019-11-08T11:44:06.637068Z"
    },
    "id": "ktcRz5Jxl7rd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.677941Z",
     "start_time": "2019-11-08T11:44:06.673660Z"
    },
    "id": "XDFPJfbOl7rd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  -------  -----\n",
      "deutsche  deutsch  ADJ\n",
      "Sprache   Sprache  NOUN\n",
      "bzw.      bzw.     CCONJ\n",
      "Deutsch   Deutsch  NOUN\n",
      "(         --       PUNCT\n",
      "[         [        PROPN\n",
      "dɔʏ̯t͡ʃ     dɔʏ̯t͡ʃ    ADV\n",
      "]         ]        PROPN\n",
      ";         --       PUNCT\n",
      "--------  -------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,0, stopwords=True)\n",
    "\n",
    "assert len(list(doc.sents)[0]) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2kKoL3Al7re"
   },
   "source": [
    "## Language detection\n",
    "\n",
    "We could also observe, that there is some English text inbetween our nice German sentences. We would like to detect foreign sentences and by later processing, ignore / skip them.\n",
    "\n",
    "For this to be achievable, we need some language detection capabilities.\n",
    "\n",
    "Luckily enough, we can make it part of our pipeline via [this extension](#https://spacy.io/universe/project/spacy-langdetect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXj9fQQIl7re"
   },
   "source": [
    "### Standard installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.709209Z",
     "start_time": "2019-11-08T11:44:06.679245Z"
    },
    "id": "GnCLOnr0l7re"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.732209Z",
     "start_time": "2019-11-08T11:44:07.714434Z"
    },
    "id": "Wa843sGal7rf"
   },
   "outputs": [],
   "source": [
    "#Please import the language detector!\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z0Af9ggl7rf"
   },
   "source": [
    "### Adding language detection to our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.736304Z",
     "start_time": "2019-11-08T11:44:07.733828Z"
    },
    "id": "fIRLLRN7l7rf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7ff5802f5f70>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please register it to the pipeline as the final step of processing!\n",
    "@Language.factory(\"language_detector\") \n",
    "\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector() \n",
    "\n",
    "nlp.add_pipe(\"language_detector\", last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MErXqXncl7rf"
   },
   "source": [
    "### Observing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.776361Z",
     "start_time": "2019-11-08T11:44:07.737797Z"
    },
    "id": "hi8X9vdkl7rf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.097242Z",
     "start_time": "2019-11-08T11:44:07.780914Z"
    },
    "id": "4KcECbZul7rg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------  ---  -----------------------------------------------\n",
      "[Die, deutsche, Sprache, bzw., Deutsch]          ...  {'language': 'de', 'score': 0.9999973314394712}\n",
      "[abgekürzt, dt, .]                               ...  {'language': 'de', 'score': 0.9999969767011094}\n",
      "[oder, dtsch, ., ), ist]                         ...  {'language': 'de', 'score': 0.9999956941901204}\n",
      "[And, this, is, an, English]                     ...  {'language': 'en', 'score': 0.9999949496238032}\n",
      "[Ihr, Sprachraum, umfasst, Deutschland, ,]       ...  {'language': 'de', 'score': 0.9999976899356049}\n",
      "[Außerdem, ist, sie, eine, Minderheitensprache]  ...  {'language': 'de', 'score': 0.9999956768203675}\n",
      "-----------------------------------------------  ---  -----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "attribs = []\n",
    "for sentence in doc.sents:\n",
    "    attribs.append([list(sentence)[:5],\"...\", sentence._.language])\n",
    "print(tabulate(attribs))\n",
    "\n",
    "# Please observe how one accesses anextension!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt3IX7kyl7rg"
   },
   "source": [
    "# Creating final generator for cleaned text\n",
    "\n",
    "Typically for a later stage of NLP, we would like to have a generator like function, which allows us to iteratively access the corpus, albeit in it's cleaned and encoded form. Integer encoding (as well as one hot encoding) are quite typical representations of text.\n",
    "\n",
    "In this spirit, we would like to implement a generator, that gives back an **array of lemmas OR lemma IDs for each sentence in the corpus, filtering out non-German sentences and punctuation / space marks**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.105282Z",
     "start_time": "2019-11-08T11:44:08.101443Z"
    },
    "id": "5vf7pv6jl7rg"
   },
   "outputs": [],
   "source": [
    "# Please implement a generator function that yields the text of the corpus as lists of sentences\n",
    "# Based on the parameters either as a list of strings or a list of IDs\n",
    "# It should filter out non-German sentences\n",
    "# as well as topwords based on lemmas\n",
    "# and punctuation and \"space like\" characters!\n",
    "\n",
    "def sentence_generator(doc, ids=False):\n",
    "    for sentence in doc.sents:\n",
    "        out_sentence = []\n",
    "        if sentence._.language['language'] == 'de':\n",
    "            for token in sentence:\n",
    "                if not token._.is_lemma_stop and token.pos_ not in {\"PUNCT\", \"SPACE\"}:\n",
    "                    if ids:\n",
    "                        out_sentence.append(token.lemma)\n",
    "                    else:\n",
    "                        out_sentence.append(token.lemma_)\n",
    "            yield out_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.144266Z",
     "start_time": "2019-11-08T11:44:08.106651Z"
    },
    "id": "rtEnUSeAl7rh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deutsch', 'Sprache', 'bzw.', 'Deutsch', '[', 'dɔʏ̯t͡ʃ', ']'] \n",
      "\n",
      "['abkürzen', 'dt'] \n",
      "\n",
      "['dtsch', 'westgermanisch', 'Sprache'] \n",
      "\n",
      "['Sprachraum', 'umfassen', 'Deutschland', 'Österreich', 'Deutschschweiz', 'Liechtenstein', 'Luxemburg', 'Ostbelgie', 'Südtirol', 'Elsass', 'Lothringen', 'Nordschleswig'] \n",
      "\n",
      "['Minderheitensprache', 'europäisch', 'außereuropäisch', 'Land', 'Beispiel', 'Rumänien', 'Südafrika', 'Nationalsprache', 'afrikanisch', 'Namibia'] \n",
      "\n",
      "[5968319817064592459, 8431935777423264011, 3072869516764223635, 13347145995516113707, 4398759943034541363, 13326684424794413812, 3806482680584466996] \n",
      "\n",
      "[12068858602874567954, 5135506797272647618] \n",
      "\n",
      "[2552743035069842888, 1774304420854013574, 8431935777423264011] \n",
      "\n",
      "[11854469037278879099, 6826961035611069329, 3491614202785599281, 16047064563126251420, 3469156011154928224, 10833980334450146958, 15216956676957942053, 5534291137827076893, 14425170055224073740, 14854674721094831692, 5682654018506929560, 10694615845175474381] \n",
      "\n",
      "[13853446524293058697, 512110525822973470, 15751849195492229329, 731233208058718707, 176351906757609250, 16018282812866072734, 14398131728093720111, 13884865873598079458, 9226656959411645728, 2911802427415368037] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sentence_generator(doc):\n",
    "    print(i,\"\\n\")\n",
    "    \n",
    "for i in sentence_generator(doc, ids=True):\n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
